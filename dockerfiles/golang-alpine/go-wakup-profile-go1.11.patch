diff --git a/src/net/http/pprof/pprof.go b/src/net/http/pprof/pprof.go
index 35b3285a08..694d3c01d2 100644
--- a/src/net/http/pprof/pprof.go
+++ b/src/net/http/pprof/pprof.go
@@ -242,6 +242,10 @@ func (name handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
 		w.Header().Set("Content-Type", "application/octet-stream")
 		w.Header().Set("Content-Disposition", fmt.Sprintf(`attachment; filename="%s"`, name))
 	}
+	rate, _ := strconv.Atoi(r.FormValue("rate"))
+	if name == "wakeup" && rate > 0 {
+		runtime.SetWakeupProfileFraction(rate)
+	}
 	p.WriteTo(w, debug)
 }
 
diff --git a/src/runtime/mprof.go b/src/runtime/mprof.go
index 43e4810d97..44ac95aaa3 100644
--- a/src/runtime/mprof.go
+++ b/src/runtime/mprof.go
@@ -23,12 +23,13 @@ const (
 	memProfile bucketType = 1 + iota
 	blockProfile
 	mutexProfile
+	wakeupProfile
 
 	// size of bucket hash table
 	buckHashSize = 179999
 
 	// max depth of stack to record in bucket
-	maxStack = 32
+	maxStack = 64
 )
 
 type bucketType int
@@ -141,6 +142,7 @@ var (
 	mbuckets  *bucket // memory profile buckets
 	bbuckets  *bucket // blocking profile buckets
 	xbuckets  *bucket // mutex profile buckets
+	wbuckets  *bucket // wakeup profile buckets
 	buckhash  *[179999]*bucket
 	bucketmem uintptr
 
@@ -154,6 +156,11 @@ var (
 		// has been flushed to the active profile.
 		flushed bool
 	}
+
+	wProf struct {
+		// Number of wakeup events. Updated atomically.
+		count uint64
+	}
 )
 
 const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 << 24)
@@ -166,7 +173,7 @@ func newBucket(typ bucketType, nstk int) *bucket {
 		throw("invalid profile bucket type")
 	case memProfile:
 		size += unsafe.Sizeof(memRecord{})
-	case blockProfile, mutexProfile:
+	case blockProfile, mutexProfile, wakeupProfile:
 		size += unsafe.Sizeof(blockRecord{})
 	}
 
@@ -194,7 +201,7 @@ func (b *bucket) mp() *memRecord {
 
 // bp returns the blockRecord associated with the blockProfile bucket b.
 func (b *bucket) bp() *blockRecord {
-	if b.typ != blockProfile && b.typ != mutexProfile {
+	if b.typ != blockProfile && b.typ != mutexProfile && b.typ != wakeupProfile {
 		throw("bad use of bucket.bp")
 	}
 	data := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(uintptr(0)))
@@ -249,6 +256,9 @@ func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket
 	} else if typ == mutexProfile {
 		b.allnext = xbuckets
 		xbuckets = b
+	} else if typ == wakeupProfile {
+		b.allnext = wbuckets
+		wbuckets = b
 	} else {
 		b.allnext = bbuckets
 		bbuckets = b
@@ -458,11 +468,63 @@ func mutexevent(cycles int64, skip int) {
 	}
 }
 
+var wakeupprofilerate uint64
+
+// SetWakeupProfileFraction controls the fraction of wake-up events
+// that are reported in the wakeup profile. On average 1/rate events are
+// reported. The previous rate is returned.
+//
+// To turn off profiling entirely, pass rate 0.
+// To just read the current rate, pass rate -1.
+// (For n>1 the details of sampling may change.)
+func SetWakeupProfileFraction(rate int) int {
+	if rate < 0 {
+		return int(wakeupprofilerate)
+	}
+	old := wakeupprofilerate
+	atomic.Store64(&wakeupprofilerate, uint64(rate))
+	return int(old)
+}
+
+func wakeupevent(gp *g, skip int) {
+	rate := uint64(atomic.Load64(&wakeupprofilerate))
+	if rate == 0 {
+		return
+	}
+	count := atomic.Xadd64(&wProf.count, 1)
+	if count % rate == 0 {
+		savewakeupevent(gp, skip)
+	}
+}
+
+func savewakeupevent(gpTo *g, skip int) {
+	var nstkFrom, nstkTo int
+	var stkFrom, stkTo [maxStack]uintptr
+	gpFrom := getg()
+	if gpFrom.m.curg == nil || gpFrom.m.curg == gpFrom {
+		nstkFrom = callers(skip, stkFrom[:])
+	} else {
+		nstkFrom = gcallers(gpFrom.m.curg, skip, stkFrom[:])
+	}
+	nstkTo = gcallers(gpTo, 0, stkTo[:])
+	cycles := cputicks()
+	lock(&proflock)
+	b := stkbucket(wakeupProfile, 0, stkFrom[:nstkFrom], true)
+	bp := b.bp()
+	bp.cycles = cycles
+	bp.count = 0
+	b = stkbucket(wakeupProfile, 0, stkTo[:nstkTo], true)
+	bp = b.bp()
+	bp.cycles = cycles
+	bp.count = 1
+	unlock(&proflock)
+}
+
 // Go interface to profile data.
 
 // A StackRecord describes a single execution stack.
 type StackRecord struct {
-	Stack0 [32]uintptr // stack trace for this record; ends at first 0 entry
+	Stack0 [64]uintptr // stack trace for this record; ends at first 0 entry
 }
 
 // Stack returns the stack trace associated with the record,
@@ -497,7 +559,7 @@ var MemProfileRate int = 512 * 1024
 type MemProfileRecord struct {
 	AllocBytes, FreeBytes     int64       // number of bytes allocated, freed
 	AllocObjects, FreeObjects int64       // number of objects allocated, freed
-	Stack0                    [32]uintptr // stack trace for this record; ends at first 0 entry
+	Stack0                    [64]uintptr // stack trace for this record; ends at first 0 entry
 }
 
 // InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).
@@ -811,6 +873,35 @@ func Stack(buf []byte, all bool) int {
 	return n
 }
 
+// WakeupProfile returns n, the number of records in the active wakeup profile.
+// If len(p) >= n, WakeupProfile copies the profile into p and returns n, true.
+// If len(p) < n, WakeupProfile does not change p and returns n, false.
+//
+// Most clients should use the runtime/pprof package instead
+// of calling WakeupProfile directly.
+func WakeupProfile(p []BlockProfileRecord) (n int, ok bool) {
+	lock(&proflock)
+	for b := wbuckets; b != nil; b = b.allnext {
+		n++
+	}
+	if n <= len(p) {
+		ok = true
+		for b := wbuckets; b != nil; b = b.allnext {
+			bp := b.bp()
+			r := &p[0]
+			r.Count = int64(bp.count)
+			r.Cycles = bp.cycles
+			i := copy(r.Stack0[:], b.stk())
+			for ; i < len(r.Stack0); i++ {
+				r.Stack0[i] = 0
+			}
+			p = p[1:]
+		}
+	}
+	unlock(&proflock)
+	return
+}
+
 // Tracing of alloc/free/gc.
 
 var tracelock mutex
diff --git a/src/runtime/pprof/pprof.go b/src/runtime/pprof/pprof.go
index c1024c99ed..e0d21fd1a5 100644
--- a/src/runtime/pprof/pprof.go
+++ b/src/runtime/pprof/pprof.go
@@ -104,6 +104,7 @@ import (
 //	threadcreate - stack traces that led to the creation of new OS threads
 //	block        - stack traces that led to blocking on synchronization primitives
 //	mutex        - stack traces of holders of contended mutexes
+//	wakeup       - stack traces of goroutines who wakes another goroutine up
 //
 // These predefined profiles maintain themselves and panic on an explicit
 // Add or Remove method call.
@@ -179,6 +180,12 @@ var mutexProfile = &Profile{
 	write: writeMutex,
 }
 
+var wakeupProfile = &Profile {
+	name: "wakeup",
+	count: countWakeup,
+	write: writeWakeup,
+}
+
 func lockProfiles() {
 	profiles.mu.Lock()
 	if profiles.m == nil {
@@ -190,6 +197,7 @@ func lockProfiles() {
 			"allocs":       allocsProfile,
 			"block":        blockProfile,
 			"mutex":        mutexProfile,
+			"wakeup":       wakeupProfile,
 		}
 	}
 }
@@ -922,3 +930,66 @@ func scaleMutexProfile(cnt int64, ns float64) (int64, float64) {
 }
 
 func runtime_cyclesPerSecond() int64
+
+// countWakeup returns the number of records in the wakeup profile.
+func countWakeup() int {
+	n, _ := runtime.WakeupProfile(nil)
+	return n
+}
+
+// writeWakeup writes the current wakeup profile to w
+func writeWakeup(w io.Writer, debug int) error {
+	var p []runtime.BlockProfileRecord
+	n, ok := runtime.WakeupProfile(nil)
+	for {
+		p = make([]runtime.BlockProfileRecord, n+50)
+		n, ok = runtime.WakeupProfile(p)
+		if ok {
+			p = p[:n]
+			break
+		}
+	}
+
+	sort.Slice(p, func(i, j int) bool {
+			return p[i].Cycles < p[j].Cycles || (p[i].Cycles == p[j].Cycles && p[i].Count < p[j].Count)
+			})
+
+	b := bufio.NewWriter(w)
+	var tw *tabwriter.Writer
+	w = b
+	if debug > 0 {
+		tw = tabwriter.NewWriter(w, 1, 8, 1, '\t', 0)
+		w = tw
+	}
+	fmt.Fprintf(w, "---wakeup:\n")
+	fmt.Fprintf(w, "sampling period=%d\n", runtime.SetWakeupProfileFraction(-1))
+	numEvents := 0
+	eventIndex := make(map[int64]int, 0)
+	for i := range p {
+		r := &p[i]
+		if _, exists := eventIndex[r.Cycles]; !exists {
+			eventIndex[r.Cycles] = numEvents
+			numEvents++
+		}
+		index := eventIndex[r.Cycles]
+		typ := "unknown"
+		switch r.Count {
+		  case 0:
+			typ = "from"
+		  case 1:
+			typ = "to"
+		}
+		fmt.Fprintf(w, "%d %s @", index, typ)
+		for _, pc := range r.Stack() {
+			fmt.Fprintf(w, " %#x", pc)
+		}
+		fmt.Fprint(w, "\n")
+		if debug > 0 {
+			printStackRecord(w, r.Stack(), true)
+		}
+	}
+	if tw != nil {
+		tw.Flush()
+	}
+	return b.Flush()
+}
diff --git a/src/runtime/proc.go b/src/runtime/proc.go
index f82014eb92..cb10643e21 100644
--- a/src/runtime/proc.go
+++ b/src/runtime/proc.go
@@ -309,6 +309,7 @@ func goparkunlock(lock *mutex, reason waitReason, traceEv byte, traceskip int) {
 }
 
 func goready(gp *g, traceskip int) {
+	wakeupevent(gp, 3)
 	systemstack(func() {
 		ready(gp, traceskip, true)
 	})
@@ -3408,6 +3409,7 @@ func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintpt
 	if trace.enabled {
 		traceGoCreate(newg, newg.startpc)
 	}
+	wakeupevent(newg, 1)
 	runqput(_p_, newg, true)
 
 	if atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 && mainStarted {
diff --git a/src/runtime/runtime2.go b/src/runtime/runtime2.go
index ad47d1275e..bcb9f5a6c9 100644
--- a/src/runtime/runtime2.go
+++ b/src/runtime/runtime2.go
@@ -443,7 +443,7 @@ type m struct {
 	schedlink     muintptr
 	mcache        *mcache
 	lockedg       guintptr
-	createstack   [32]uintptr    // stack that created this thread.
+	createstack   [64]uintptr    // stack that created this thread.
 	lockedExt     uint32         // tracking for external LockOSThread
 	lockedInt     uint32         // tracking for internal lockOSThread
 	nextwaitm     muintptr       // next m waiting for lock
